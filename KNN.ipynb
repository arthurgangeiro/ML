{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics  import   accuracy_score \n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_teste(predictions, alg_name):\n",
    "\n",
    "    print('Resultados para o classificador {0}:'.format(alg_name))\n",
    "    print(classification_report(y_teste, predictions), \n",
    "    print (\"Acurácia para o treino é \", accuracy_score(y_teste,predictions)))\n",
    "    \n",
    "def report_treino(predictions, alg_name):\n",
    "\n",
    "    print('Resultados para o classificador {0}:'.format(alg_name))\n",
    "    print(classification_report(y_treino, predictions), \n",
    "    print (\"Acurácia para o treino é \", accuracy_score(y_treino,predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('C:\\\\Users\\\\Fabiel Fernando\\\\Desktop\\\\PROVA\\\\classificacao_Q4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificando a existência de missings\n",
    "#dataset.apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "      <th>x16</th>\n",
       "      <th>x17</th>\n",
       "      <th>x18</th>\n",
       "      <th>x19</th>\n",
       "      <th>x20</th>\n",
       "      <th>x21</th>\n",
       "      <th>x22</th>\n",
       "      <th>x23</th>\n",
       "      <th>x24</th>\n",
       "      <th>x25</th>\n",
       "      <th>x26</th>\n",
       "      <th>x27</th>\n",
       "      <th>x28</th>\n",
       "      <th>x29</th>\n",
       "      <th>x30</th>\n",
       "      <th>x31</th>\n",
       "      <th>x32</th>\n",
       "      <th>x33</th>\n",
       "      <th>x34</th>\n",
       "      <th>x35</th>\n",
       "      <th>x36</th>\n",
       "      <th>x37</th>\n",
       "      <th>x38</th>\n",
       "      <th>x39</th>\n",
       "      <th>x40</th>\n",
       "      <th>x41</th>\n",
       "      <th>x42</th>\n",
       "      <th>x43</th>\n",
       "      <th>x44</th>\n",
       "      <th>x45</th>\n",
       "      <th>x46</th>\n",
       "      <th>x47</th>\n",
       "      <th>x48</th>\n",
       "      <th>x49</th>\n",
       "      <th>x50</th>\n",
       "      <th>x51</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "      <th>x62</th>\n",
       "      <th>x63</th>\n",
       "      <th>x64</th>\n",
       "      <th>x65</th>\n",
       "      <th>x66</th>\n",
       "      <th>x67</th>\n",
       "      <th>x68</th>\n",
       "      <th>x69</th>\n",
       "      <th>x70</th>\n",
       "      <th>x71</th>\n",
       "      <th>x72</th>\n",
       "      <th>x73</th>\n",
       "      <th>x74</th>\n",
       "      <th>x75</th>\n",
       "      <th>x76</th>\n",
       "      <th>x77</th>\n",
       "      <th>x78</th>\n",
       "      <th>x79</th>\n",
       "      <th>x80</th>\n",
       "      <th>x81</th>\n",
       "      <th>x82</th>\n",
       "      <th>x83</th>\n",
       "      <th>x84</th>\n",
       "      <th>x85</th>\n",
       "      <th>x86</th>\n",
       "      <th>x87</th>\n",
       "      <th>x88</th>\n",
       "      <th>x89</th>\n",
       "      <th>x90</th>\n",
       "      <th>x91</th>\n",
       "      <th>x92</th>\n",
       "      <th>x93</th>\n",
       "      <th>x94</th>\n",
       "      <th>x95</th>\n",
       "      <th>x96</th>\n",
       "      <th>x97</th>\n",
       "      <th>x98</th>\n",
       "      <th>x99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.696199</td>\n",
       "      <td>-0.792598</td>\n",
       "      <td>-0.349427</td>\n",
       "      <td>-0.464560</td>\n",
       "      <td>3.187014</td>\n",
       "      <td>0.035976</td>\n",
       "      <td>1.033274</td>\n",
       "      <td>-1.504968</td>\n",
       "      <td>0.204693</td>\n",
       "      <td>1.691204</td>\n",
       "      <td>-0.148668</td>\n",
       "      <td>-4.074097</td>\n",
       "      <td>-0.032896</td>\n",
       "      <td>-0.663494</td>\n",
       "      <td>-0.386016</td>\n",
       "      <td>-0.237805</td>\n",
       "      <td>-1.510523</td>\n",
       "      <td>-1.570864</td>\n",
       "      <td>-0.368605</td>\n",
       "      <td>0.812503</td>\n",
       "      <td>0.549905</td>\n",
       "      <td>-0.730260</td>\n",
       "      <td>0.761423</td>\n",
       "      <td>1.128273</td>\n",
       "      <td>-1.763750</td>\n",
       "      <td>0.579692</td>\n",
       "      <td>-0.293674</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>-0.427231</td>\n",
       "      <td>-0.295434</td>\n",
       "      <td>-2.626552</td>\n",
       "      <td>-0.888908</td>\n",
       "      <td>0.360110</td>\n",
       "      <td>-3.085644</td>\n",
       "      <td>-0.945316</td>\n",
       "      <td>-0.904486</td>\n",
       "      <td>1.072223</td>\n",
       "      <td>1.778115</td>\n",
       "      <td>-0.148051</td>\n",
       "      <td>0.634574</td>\n",
       "      <td>0.209628</td>\n",
       "      <td>0.561244</td>\n",
       "      <td>-0.586968</td>\n",
       "      <td>-3.702351</td>\n",
       "      <td>-0.649087</td>\n",
       "      <td>0.066648</td>\n",
       "      <td>0.521637</td>\n",
       "      <td>-0.318873</td>\n",
       "      <td>-0.964632</td>\n",
       "      <td>-0.068293</td>\n",
       "      <td>-1.941717</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>-0.030974</td>\n",
       "      <td>1.666534</td>\n",
       "      <td>1.907174</td>\n",
       "      <td>0.454065</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>-1.415378</td>\n",
       "      <td>-0.220428</td>\n",
       "      <td>-1.163591</td>\n",
       "      <td>0.643701</td>\n",
       "      <td>-0.593975</td>\n",
       "      <td>-0.230020</td>\n",
       "      <td>2.142668</td>\n",
       "      <td>-1.150896</td>\n",
       "      <td>1.980677</td>\n",
       "      <td>1.115755</td>\n",
       "      <td>0.511176</td>\n",
       "      <td>-0.526043</td>\n",
       "      <td>-0.492225</td>\n",
       "      <td>1.291322</td>\n",
       "      <td>-0.795223</td>\n",
       "      <td>1.292448</td>\n",
       "      <td>0.804562</td>\n",
       "      <td>0.822480</td>\n",
       "      <td>-1.205006</td>\n",
       "      <td>-0.280887</td>\n",
       "      <td>-1.364098</td>\n",
       "      <td>0.312000</td>\n",
       "      <td>-1.925461</td>\n",
       "      <td>0.498012</td>\n",
       "      <td>0.371394</td>\n",
       "      <td>0.176175</td>\n",
       "      <td>0.547430</td>\n",
       "      <td>1.058247</td>\n",
       "      <td>0.503351</td>\n",
       "      <td>1.018997</td>\n",
       "      <td>0.221213</td>\n",
       "      <td>-0.419000</td>\n",
       "      <td>-0.858737</td>\n",
       "      <td>-0.534360</td>\n",
       "      <td>1.488142</td>\n",
       "      <td>-0.686337</td>\n",
       "      <td>2.084970</td>\n",
       "      <td>-0.685140</td>\n",
       "      <td>-2.049451</td>\n",
       "      <td>2.015426</td>\n",
       "      <td>1.158477</td>\n",
       "      <td>-0.309441</td>\n",
       "      <td>-1.549833</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.236696</td>\n",
       "      <td>-2.202342</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>1.497700</td>\n",
       "      <td>-0.069758</td>\n",
       "      <td>-2.467088</td>\n",
       "      <td>1.126529</td>\n",
       "      <td>-0.570557</td>\n",
       "      <td>2.079251</td>\n",
       "      <td>-1.882632</td>\n",
       "      <td>-0.827576</td>\n",
       "      <td>1.005103</td>\n",
       "      <td>-0.137394</td>\n",
       "      <td>1.189628</td>\n",
       "      <td>-0.851586</td>\n",
       "      <td>-1.288871</td>\n",
       "      <td>-0.963559</td>\n",
       "      <td>1.227582</td>\n",
       "      <td>0.715197</td>\n",
       "      <td>0.520097</td>\n",
       "      <td>0.588903</td>\n",
       "      <td>-0.590111</td>\n",
       "      <td>-2.210356</td>\n",
       "      <td>1.022461</td>\n",
       "      <td>-1.039452</td>\n",
       "      <td>-0.241972</td>\n",
       "      <td>0.282824</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>-1.621286</td>\n",
       "      <td>-1.815760</td>\n",
       "      <td>0.663234</td>\n",
       "      <td>-0.208910</td>\n",
       "      <td>0.113045</td>\n",
       "      <td>2.046566</td>\n",
       "      <td>0.761385</td>\n",
       "      <td>1.412045</td>\n",
       "      <td>2.094611</td>\n",
       "      <td>-0.286475</td>\n",
       "      <td>0.718189</td>\n",
       "      <td>-0.421027</td>\n",
       "      <td>1.182153</td>\n",
       "      <td>0.379603</td>\n",
       "      <td>-0.835262</td>\n",
       "      <td>0.937721</td>\n",
       "      <td>0.114378</td>\n",
       "      <td>-0.651730</td>\n",
       "      <td>-0.047160</td>\n",
       "      <td>3.589095</td>\n",
       "      <td>-0.486826</td>\n",
       "      <td>2.847869</td>\n",
       "      <td>0.162564</td>\n",
       "      <td>-0.039426</td>\n",
       "      <td>0.462479</td>\n",
       "      <td>-1.531158</td>\n",
       "      <td>-1.860289</td>\n",
       "      <td>0.455750</td>\n",
       "      <td>2.220489</td>\n",
       "      <td>1.212844</td>\n",
       "      <td>-1.329690</td>\n",
       "      <td>-1.452428</td>\n",
       "      <td>0.053086</td>\n",
       "      <td>-0.574263</td>\n",
       "      <td>-2.518650</td>\n",
       "      <td>-1.737640</td>\n",
       "      <td>-0.194589</td>\n",
       "      <td>0.648973</td>\n",
       "      <td>-0.342163</td>\n",
       "      <td>-0.508209</td>\n",
       "      <td>0.947281</td>\n",
       "      <td>-0.430554</td>\n",
       "      <td>0.661217</td>\n",
       "      <td>-1.936414</td>\n",
       "      <td>-1.698198</td>\n",
       "      <td>-3.313671</td>\n",
       "      <td>-0.183713</td>\n",
       "      <td>-0.549041</td>\n",
       "      <td>1.280620</td>\n",
       "      <td>2.177973</td>\n",
       "      <td>0.706155</td>\n",
       "      <td>-1.002186</td>\n",
       "      <td>-0.760492</td>\n",
       "      <td>0.390230</td>\n",
       "      <td>1.652978</td>\n",
       "      <td>-0.281058</td>\n",
       "      <td>-2.274763</td>\n",
       "      <td>-1.451749</td>\n",
       "      <td>-0.594344</td>\n",
       "      <td>1.292452</td>\n",
       "      <td>1.066120</td>\n",
       "      <td>0.036062</td>\n",
       "      <td>0.498207</td>\n",
       "      <td>0.405567</td>\n",
       "      <td>0.509564</td>\n",
       "      <td>1.374071</td>\n",
       "      <td>-0.016943</td>\n",
       "      <td>-0.429280</td>\n",
       "      <td>-0.895016</td>\n",
       "      <td>1.259566</td>\n",
       "      <td>-0.354139</td>\n",
       "      <td>0.806797</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.436683</td>\n",
       "      <td>1.563816</td>\n",
       "      <td>-0.895999</td>\n",
       "      <td>-0.580425</td>\n",
       "      <td>0.311060</td>\n",
       "      <td>-0.187369</td>\n",
       "      <td>0.805249</td>\n",
       "      <td>-2.399522</td>\n",
       "      <td>-0.578818</td>\n",
       "      <td>1.586981</td>\n",
       "      <td>-1.941955</td>\n",
       "      <td>-0.596377</td>\n",
       "      <td>-0.489321</td>\n",
       "      <td>-1.030148</td>\n",
       "      <td>-0.485569</td>\n",
       "      <td>0.902347</td>\n",
       "      <td>0.107147</td>\n",
       "      <td>-0.780838</td>\n",
       "      <td>0.402332</td>\n",
       "      <td>-1.450170</td>\n",
       "      <td>-0.583627</td>\n",
       "      <td>-0.706544</td>\n",
       "      <td>-0.025883</td>\n",
       "      <td>-1.450107</td>\n",
       "      <td>2.118729</td>\n",
       "      <td>1.015845</td>\n",
       "      <td>0.166787</td>\n",
       "      <td>-0.044010</td>\n",
       "      <td>-0.360155</td>\n",
       "      <td>0.101155</td>\n",
       "      <td>-0.799201</td>\n",
       "      <td>-1.102617</td>\n",
       "      <td>2.115397</td>\n",
       "      <td>-2.361777</td>\n",
       "      <td>0.525674</td>\n",
       "      <td>-1.911165</td>\n",
       "      <td>0.123961</td>\n",
       "      <td>-0.417771</td>\n",
       "      <td>0.548105</td>\n",
       "      <td>-0.217684</td>\n",
       "      <td>-0.431924</td>\n",
       "      <td>-0.442644</td>\n",
       "      <td>-1.489144</td>\n",
       "      <td>-1.000744</td>\n",
       "      <td>0.862522</td>\n",
       "      <td>-0.563455</td>\n",
       "      <td>0.588636</td>\n",
       "      <td>0.010576</td>\n",
       "      <td>-0.456408</td>\n",
       "      <td>-1.428348</td>\n",
       "      <td>0.216525</td>\n",
       "      <td>1.290350</td>\n",
       "      <td>-1.092070</td>\n",
       "      <td>0.522418</td>\n",
       "      <td>2.553921</td>\n",
       "      <td>0.087687</td>\n",
       "      <td>1.755408</td>\n",
       "      <td>-1.382265</td>\n",
       "      <td>0.032006</td>\n",
       "      <td>0.680842</td>\n",
       "      <td>0.911192</td>\n",
       "      <td>0.505370</td>\n",
       "      <td>-0.741637</td>\n",
       "      <td>0.980315</td>\n",
       "      <td>2.359120</td>\n",
       "      <td>-0.380329</td>\n",
       "      <td>0.234811</td>\n",
       "      <td>2.287361</td>\n",
       "      <td>-0.568738</td>\n",
       "      <td>-1.932310</td>\n",
       "      <td>-1.912456</td>\n",
       "      <td>-1.829811</td>\n",
       "      <td>-0.589138</td>\n",
       "      <td>0.473086</td>\n",
       "      <td>-0.237060</td>\n",
       "      <td>-0.106093</td>\n",
       "      <td>-0.690060</td>\n",
       "      <td>-0.640960</td>\n",
       "      <td>-1.088658</td>\n",
       "      <td>-0.998397</td>\n",
       "      <td>-1.579437</td>\n",
       "      <td>-0.697638</td>\n",
       "      <td>-0.620487</td>\n",
       "      <td>-0.320028</td>\n",
       "      <td>1.390414</td>\n",
       "      <td>0.449638</td>\n",
       "      <td>0.300941</td>\n",
       "      <td>-0.512526</td>\n",
       "      <td>0.656667</td>\n",
       "      <td>0.265771</td>\n",
       "      <td>-2.630024</td>\n",
       "      <td>0.933578</td>\n",
       "      <td>-1.285978</td>\n",
       "      <td>0.503162</td>\n",
       "      <td>0.204829</td>\n",
       "      <td>-0.753835</td>\n",
       "      <td>0.290033</td>\n",
       "      <td>1.721487</td>\n",
       "      <td>1.304518</td>\n",
       "      <td>0.478903</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.425908</td>\n",
       "      <td>0.400055</td>\n",
       "      <td>-0.305038</td>\n",
       "      <td>-0.930251</td>\n",
       "      <td>-2.214549</td>\n",
       "      <td>1.763379</td>\n",
       "      <td>-0.239868</td>\n",
       "      <td>-2.058891</td>\n",
       "      <td>-1.006533</td>\n",
       "      <td>-2.156839</td>\n",
       "      <td>-0.817310</td>\n",
       "      <td>3.135035</td>\n",
       "      <td>-1.046031</td>\n",
       "      <td>2.035231</td>\n",
       "      <td>0.307369</td>\n",
       "      <td>-0.831289</td>\n",
       "      <td>-0.263652</td>\n",
       "      <td>-1.479070</td>\n",
       "      <td>-0.675276</td>\n",
       "      <td>-0.222479</td>\n",
       "      <td>-0.441100</td>\n",
       "      <td>0.343649</td>\n",
       "      <td>0.210042</td>\n",
       "      <td>-2.030159</td>\n",
       "      <td>0.636847</td>\n",
       "      <td>-2.268783</td>\n",
       "      <td>1.066813</td>\n",
       "      <td>1.486655</td>\n",
       "      <td>0.665269</td>\n",
       "      <td>1.207031</td>\n",
       "      <td>3.549965</td>\n",
       "      <td>-0.026904</td>\n",
       "      <td>1.027441</td>\n",
       "      <td>1.979429</td>\n",
       "      <td>1.133188</td>\n",
       "      <td>1.709450</td>\n",
       "      <td>1.046510</td>\n",
       "      <td>1.397032</td>\n",
       "      <td>0.177327</td>\n",
       "      <td>-0.402179</td>\n",
       "      <td>-0.054244</td>\n",
       "      <td>-0.578126</td>\n",
       "      <td>-0.055127</td>\n",
       "      <td>2.794188</td>\n",
       "      <td>0.528181</td>\n",
       "      <td>-0.140851</td>\n",
       "      <td>-0.320488</td>\n",
       "      <td>-0.552952</td>\n",
       "      <td>-2.406692</td>\n",
       "      <td>0.054562</td>\n",
       "      <td>0.886823</td>\n",
       "      <td>-0.419061</td>\n",
       "      <td>-0.272393</td>\n",
       "      <td>-2.141239</td>\n",
       "      <td>-0.114749</td>\n",
       "      <td>0.230638</td>\n",
       "      <td>-0.250862</td>\n",
       "      <td>1.116209</td>\n",
       "      <td>1.452902</td>\n",
       "      <td>0.927677</td>\n",
       "      <td>-0.136729</td>\n",
       "      <td>-0.873607</td>\n",
       "      <td>0.430335</td>\n",
       "      <td>0.828970</td>\n",
       "      <td>0.313719</td>\n",
       "      <td>0.378332</td>\n",
       "      <td>-0.586515</td>\n",
       "      <td>-1.448876</td>\n",
       "      <td>-0.149765</td>\n",
       "      <td>-0.958114</td>\n",
       "      <td>-1.478115</td>\n",
       "      <td>-2.388252</td>\n",
       "      <td>-1.569214</td>\n",
       "      <td>-2.755844</td>\n",
       "      <td>-1.098166</td>\n",
       "      <td>1.450431</td>\n",
       "      <td>1.134263</td>\n",
       "      <td>2.586703</td>\n",
       "      <td>-0.224750</td>\n",
       "      <td>-0.036701</td>\n",
       "      <td>2.264622</td>\n",
       "      <td>-0.035200</td>\n",
       "      <td>0.217302</td>\n",
       "      <td>0.038805</td>\n",
       "      <td>-0.604043</td>\n",
       "      <td>-1.798876</td>\n",
       "      <td>-2.307973</td>\n",
       "      <td>1.441341</td>\n",
       "      <td>2.311820</td>\n",
       "      <td>-0.947016</td>\n",
       "      <td>-0.260665</td>\n",
       "      <td>-0.849927</td>\n",
       "      <td>1.402768</td>\n",
       "      <td>0.393653</td>\n",
       "      <td>-1.466818</td>\n",
       "      <td>0.152257</td>\n",
       "      <td>-4.004950</td>\n",
       "      <td>0.676342</td>\n",
       "      <td>-1.927319</td>\n",
       "      <td>1.959032</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.186156</td>\n",
       "      <td>-0.975764</td>\n",
       "      <td>0.594660</td>\n",
       "      <td>-1.181980</td>\n",
       "      <td>-1.443414</td>\n",
       "      <td>-0.797651</td>\n",
       "      <td>-1.252608</td>\n",
       "      <td>-0.060452</td>\n",
       "      <td>0.130702</td>\n",
       "      <td>-2.343517</td>\n",
       "      <td>0.892393</td>\n",
       "      <td>-0.533092</td>\n",
       "      <td>-0.760388</td>\n",
       "      <td>-0.702277</td>\n",
       "      <td>0.259456</td>\n",
       "      <td>3.732211</td>\n",
       "      <td>1.185647</td>\n",
       "      <td>2.046445</td>\n",
       "      <td>-1.378246</td>\n",
       "      <td>-0.733557</td>\n",
       "      <td>4.716702</td>\n",
       "      <td>0.229157</td>\n",
       "      <td>1.955133</td>\n",
       "      <td>1.917857</td>\n",
       "      <td>-1.783127</td>\n",
       "      <td>-0.839499</td>\n",
       "      <td>-1.811106</td>\n",
       "      <td>-0.405222</td>\n",
       "      <td>0.074332</td>\n",
       "      <td>2.034061</td>\n",
       "      <td>0.179220</td>\n",
       "      <td>-0.458617</td>\n",
       "      <td>-3.470883</td>\n",
       "      <td>0.561481</td>\n",
       "      <td>0.492969</td>\n",
       "      <td>1.310855</td>\n",
       "      <td>0.505790</td>\n",
       "      <td>-1.135986</td>\n",
       "      <td>-0.696156</td>\n",
       "      <td>0.815568</td>\n",
       "      <td>-0.266634</td>\n",
       "      <td>0.245124</td>\n",
       "      <td>1.244601</td>\n",
       "      <td>0.930504</td>\n",
       "      <td>-2.423524</td>\n",
       "      <td>-0.217978</td>\n",
       "      <td>-0.250712</td>\n",
       "      <td>-0.180181</td>\n",
       "      <td>1.579620</td>\n",
       "      <td>-1.239677</td>\n",
       "      <td>-0.917660</td>\n",
       "      <td>1.345773</td>\n",
       "      <td>0.545109</td>\n",
       "      <td>2.444263</td>\n",
       "      <td>-1.244190</td>\n",
       "      <td>0.446668</td>\n",
       "      <td>0.178714</td>\n",
       "      <td>-0.714363</td>\n",
       "      <td>0.310813</td>\n",
       "      <td>-4.723429</td>\n",
       "      <td>1.025380</td>\n",
       "      <td>0.567891</td>\n",
       "      <td>-1.215820</td>\n",
       "      <td>0.061255</td>\n",
       "      <td>1.798139</td>\n",
       "      <td>-0.254473</td>\n",
       "      <td>0.091907</td>\n",
       "      <td>0.680257</td>\n",
       "      <td>1.232538</td>\n",
       "      <td>-0.482364</td>\n",
       "      <td>1.012526</td>\n",
       "      <td>-0.554645</td>\n",
       "      <td>0.451229</td>\n",
       "      <td>0.484063</td>\n",
       "      <td>2.466720</td>\n",
       "      <td>0.102488</td>\n",
       "      <td>-0.574971</td>\n",
       "      <td>-2.885352</td>\n",
       "      <td>0.911710</td>\n",
       "      <td>-0.846603</td>\n",
       "      <td>0.850602</td>\n",
       "      <td>2.222440</td>\n",
       "      <td>-1.981894</td>\n",
       "      <td>0.156248</td>\n",
       "      <td>-2.788302</td>\n",
       "      <td>-0.067919</td>\n",
       "      <td>1.352606</td>\n",
       "      <td>-1.878879</td>\n",
       "      <td>-0.943184</td>\n",
       "      <td>-0.185896</td>\n",
       "      <td>1.098563</td>\n",
       "      <td>-1.444435</td>\n",
       "      <td>-1.818126</td>\n",
       "      <td>0.446574</td>\n",
       "      <td>0.239328</td>\n",
       "      <td>0.802939</td>\n",
       "      <td>-2.035289</td>\n",
       "      <td>-1.433793</td>\n",
       "      <td>-0.218596</td>\n",
       "      <td>0.619317</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x0        x1        x2        x3        x4        x5        x6  \\\n",
       "0  1.696199 -0.792598 -0.349427 -0.464560  3.187014  0.035976  1.033274   \n",
       "1 -0.236696 -2.202342  0.024023  1.497700 -0.069758 -2.467088  1.126529   \n",
       "2 -0.436683  1.563816 -0.895999 -0.580425  0.311060 -0.187369  0.805249   \n",
       "3  1.425908  0.400055 -0.305038 -0.930251 -2.214549  1.763379 -0.239868   \n",
       "4 -0.186156 -0.975764  0.594660 -1.181980 -1.443414 -0.797651 -1.252608   \n",
       "\n",
       "         x7        x8        x9       x10       x11       x12       x13  \\\n",
       "0 -1.504968  0.204693  1.691204 -0.148668 -4.074097 -0.032896 -0.663494   \n",
       "1 -0.570557  2.079251 -1.882632 -0.827576  1.005103 -0.137394  1.189628   \n",
       "2 -2.399522 -0.578818  1.586981 -1.941955 -0.596377 -0.489321 -1.030148   \n",
       "3 -2.058891 -1.006533 -2.156839 -0.817310  3.135035 -1.046031  2.035231   \n",
       "4 -0.060452  0.130702 -2.343517  0.892393 -0.533092 -0.760388 -0.702277   \n",
       "\n",
       "        x14       x15       x16       x17       x18       x19       x20  \\\n",
       "0 -0.386016 -0.237805 -1.510523 -1.570864 -0.368605  0.812503  0.549905   \n",
       "1 -0.851586 -1.288871 -0.963559  1.227582  0.715197  0.520097  0.588903   \n",
       "2 -0.485569  0.902347  0.107147 -0.780838  0.402332 -1.450170 -0.583627   \n",
       "3  0.307369 -0.831289 -0.263652 -1.479070 -0.675276 -0.222479 -0.441100   \n",
       "4  0.259456  3.732211  1.185647  2.046445 -1.378246 -0.733557  4.716702   \n",
       "\n",
       "        x21       x22       x23       x24       x25       x26       x27  \\\n",
       "0 -0.730260  0.761423  1.128273 -1.763750  0.579692 -0.293674  0.295500   \n",
       "1 -0.590111 -2.210356  1.022461 -1.039452 -0.241972  0.282824  0.001147   \n",
       "2 -0.706544 -0.025883 -1.450107  2.118729  1.015845  0.166787 -0.044010   \n",
       "3  0.343649  0.210042 -2.030159  0.636847 -2.268783  1.066813  1.486655   \n",
       "4  0.229157  1.955133  1.917857 -1.783127 -0.839499 -1.811106 -0.405222   \n",
       "\n",
       "        x28       x29       x30       x31       x32       x33       x34  \\\n",
       "0 -0.427231 -0.295434 -2.626552 -0.888908  0.360110 -3.085644 -0.945316   \n",
       "1 -1.621286 -1.815760  0.663234 -0.208910  0.113045  2.046566  0.761385   \n",
       "2 -0.360155  0.101155 -0.799201 -1.102617  2.115397 -2.361777  0.525674   \n",
       "3  0.665269  1.207031  3.549965 -0.026904  1.027441  1.979429  1.133188   \n",
       "4  0.074332  2.034061  0.179220 -0.458617 -3.470883  0.561481  0.492969   \n",
       "\n",
       "        x35       x36       x37       x38       x39       x40       x41  \\\n",
       "0 -0.904486  1.072223  1.778115 -0.148051  0.634574  0.209628  0.561244   \n",
       "1  1.412045  2.094611 -0.286475  0.718189 -0.421027  1.182153  0.379603   \n",
       "2 -1.911165  0.123961 -0.417771  0.548105 -0.217684 -0.431924 -0.442644   \n",
       "3  1.709450  1.046510  1.397032  0.177327 -0.402179 -0.054244 -0.578126   \n",
       "4  1.310855  0.505790 -1.135986 -0.696156  0.815568 -0.266634  0.245124   \n",
       "\n",
       "        x42       x43       x44       x45       x46       x47       x48  \\\n",
       "0 -0.586968 -3.702351 -0.649087  0.066648  0.521637 -0.318873 -0.964632   \n",
       "1 -0.835262  0.937721  0.114378 -0.651730 -0.047160  3.589095 -0.486826   \n",
       "2 -1.489144 -1.000744  0.862522 -0.563455  0.588636  0.010576 -0.456408   \n",
       "3 -0.055127  2.794188  0.528181 -0.140851 -0.320488 -0.552952 -2.406692   \n",
       "4  1.244601  0.930504 -2.423524 -0.217978 -0.250712 -0.180181  1.579620   \n",
       "\n",
       "        x49       x50       x51       x52       x53       x54       x55  \\\n",
       "0 -0.068293 -1.941717  0.011300 -0.030974  1.666534  1.907174  0.454065   \n",
       "1  2.847869  0.162564 -0.039426  0.462479 -1.531158 -1.860289  0.455750   \n",
       "2 -1.428348  0.216525  1.290350 -1.092070  0.522418  2.553921  0.087687   \n",
       "3  0.054562  0.886823 -0.419061 -0.272393 -2.141239 -0.114749  0.230638   \n",
       "4 -1.239677 -0.917660  1.345773  0.545109  2.444263 -1.244190  0.446668   \n",
       "\n",
       "        x56       x57       x58       x59       x60       x61       x62  \\\n",
       "0  0.157899 -1.415378 -0.220428 -1.163591  0.643701 -0.593975 -0.230020   \n",
       "1  2.220489  1.212844 -1.329690 -1.452428  0.053086 -0.574263 -2.518650   \n",
       "2  1.755408 -1.382265  0.032006  0.680842  0.911192  0.505370 -0.741637   \n",
       "3 -0.250862  1.116209  1.452902  0.927677 -0.136729 -0.873607  0.430335   \n",
       "4  0.178714 -0.714363  0.310813 -4.723429  1.025380  0.567891 -1.215820   \n",
       "\n",
       "        x63       x64       x65       x66       x67       x68       x69  \\\n",
       "0  2.142668 -1.150896  1.980677  1.115755  0.511176 -0.526043 -0.492225   \n",
       "1 -1.737640 -0.194589  0.648973 -0.342163 -0.508209  0.947281 -0.430554   \n",
       "2  0.980315  2.359120 -0.380329  0.234811  2.287361 -0.568738 -1.932310   \n",
       "3  0.828970  0.313719  0.378332 -0.586515 -1.448876 -0.149765 -0.958114   \n",
       "4  0.061255  1.798139 -0.254473  0.091907  0.680257  1.232538 -0.482364   \n",
       "\n",
       "        x70       x71       x72       x73       x74       x75       x76  \\\n",
       "0  1.291322 -0.795223  1.292448  0.804562  0.822480 -1.205006 -0.280887   \n",
       "1  0.661217 -1.936414 -1.698198 -3.313671 -0.183713 -0.549041  1.280620   \n",
       "2 -1.912456 -1.829811 -0.589138  0.473086 -0.237060 -0.106093 -0.690060   \n",
       "3 -1.478115 -2.388252 -1.569214 -2.755844 -1.098166  1.450431  1.134263   \n",
       "4  1.012526 -0.554645  0.451229  0.484063  2.466720  0.102488 -0.574971   \n",
       "\n",
       "        x77       x78       x79       x80       x81       x82       x83  \\\n",
       "0 -1.364098  0.312000 -1.925461  0.498012  0.371394  0.176175  0.547430   \n",
       "1  2.177973  0.706155 -1.002186 -0.760492  0.390230  1.652978 -0.281058   \n",
       "2 -0.640960 -1.088658 -0.998397 -1.579437 -0.697638 -0.620487 -0.320028   \n",
       "3  2.586703 -0.224750 -0.036701  2.264622 -0.035200  0.217302  0.038805   \n",
       "4 -2.885352  0.911710 -0.846603  0.850602  2.222440 -1.981894  0.156248   \n",
       "\n",
       "        x84       x85       x86       x87       x88       x89       x90  \\\n",
       "0  1.058247  0.503351  1.018997  0.221213 -0.419000 -0.858737 -0.534360   \n",
       "1 -2.274763 -1.451749 -0.594344  1.292452  1.066120  0.036062  0.498207   \n",
       "2  1.390414  0.449638  0.300941 -0.512526  0.656667  0.265771 -2.630024   \n",
       "3 -0.604043 -1.798876 -2.307973  1.441341  2.311820 -0.947016 -0.260665   \n",
       "4 -2.788302 -0.067919  1.352606 -1.878879 -0.943184 -0.185896  1.098563   \n",
       "\n",
       "        x91       x92       x93       x94       x95       x96       x97  \\\n",
       "0  1.488142 -0.686337  2.084970 -0.685140 -2.049451  2.015426  1.158477   \n",
       "1  0.405567  0.509564  1.374071 -0.016943 -0.429280 -0.895016  1.259566   \n",
       "2  0.933578 -1.285978  0.503162  0.204829 -0.753835  0.290033  1.721487   \n",
       "3 -0.849927  1.402768  0.393653 -1.466818  0.152257 -4.004950  0.676342   \n",
       "4 -1.444435 -1.818126  0.446574  0.239328  0.802939 -2.035289 -1.433793   \n",
       "\n",
       "        x98       x99  target  \n",
       "0 -0.309441 -1.549833     4.0  \n",
       "1 -0.354139  0.806797     5.0  \n",
       "2  1.304518  0.478903     3.0  \n",
       "3 -1.927319  1.959032     8.0  \n",
       "4 -0.218596  0.619317     9.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensão dos nossos dados:\n",
      " (1500, 101)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensão dos nossos dados:\\n\", \n",
    "     dataset.shape)\n",
    "#print(\"Tipo de variáveis:\\n\",\n",
    "#     dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pocentagem da variável resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># target</th>\n",
       "      <th>% target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>153</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.0</th>\n",
       "      <td>153</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>152</td>\n",
       "      <td>10.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>150</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>150</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>149</td>\n",
       "      <td>9.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>149</td>\n",
       "      <td>9.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9.0</th>\n",
       "      <td>149</td>\n",
       "      <td>9.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>148</td>\n",
       "      <td>9.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>147</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     # target   % target\n",
       "2.0       153  10.200000\n",
       "8.0       153  10.200000\n",
       "3.0       152  10.133333\n",
       "0.0       150  10.000000\n",
       "4.0       150  10.000000\n",
       "1.0       149   9.933333\n",
       "6.0       149   9.933333\n",
       "9.0       149   9.933333\n",
       "5.0       148   9.866667\n",
       "7.0       147   9.800000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta = dataset['target']\n",
    "count = pd.DataFrame(resposta.value_counts())\n",
    "percent = pd.DataFrame(resposta.value_counts(normalize = True)*100)\n",
    "table = pd.concat([count, percent], axis = 1)\n",
    "table.columns = ['# target', '% target']\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descritiva de algumas variáveis\n",
    "#dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_space = dataset.iloc[:, dataset.columns != 'target']\n",
    "feature_class = dataset.iloc[:, dataset.columns == 'target']\n",
    "\n",
    "\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(feature_space,\n",
    "                                                                    feature_class,\n",
    "                                                                    test_size = 0.30, \n",
    "                                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpar conjuntos de teste para evitar futuras mensagens de aviso\n",
    "y_treino = y_treino.values.ravel() \n",
    "y_teste = y_teste.values.ravel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustando KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier(n_neighbors=5,\n",
    "                                  weights='uniform',\n",
    "                                  algorithm='auto',\n",
    "                                  leaf_size=30,\n",
    "                                  p=2,\n",
    "                                  metric='minkowski',\n",
    "                                  metric_params=None,\n",
    "                                  n_jobs=1)\n",
    "classifier.fit(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precisão do classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = classifier.predict(X_teste)\n",
    "pred_train = classifier.predict(X_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabela com cálculo de vária métricas conjunto treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador KNN:\n",
      "Acurácia para o treino é  0.7285714285714285\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      0.74      0.72       103\n",
      "        1.0       0.64      0.82      0.72        98\n",
      "        2.0       0.69      0.64      0.66       111\n",
      "        3.0       0.89      0.79      0.84       105\n",
      "        4.0       0.77      0.74      0.75       104\n",
      "        5.0       0.67      0.55      0.60        97\n",
      "        6.0       0.65      0.64      0.65        98\n",
      "        7.0       0.76      0.73      0.75       111\n",
      "        8.0       0.84      0.76      0.80       106\n",
      "        9.0       0.70      0.85      0.77       117\n",
      "\n",
      "avg / total       0.73      0.73      0.73      1050\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "report_treino(pred_train,'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabela com cálculo de vária métricas conjunto teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador KNN:\n",
      "Acurácia para o treino é  0.5977777777777777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.63      0.62      0.62        47\n",
      "        1.0       0.58      0.69      0.63        51\n",
      "        2.0       0.41      0.50      0.45        42\n",
      "        3.0       0.75      0.57      0.65        47\n",
      "        4.0       0.72      0.61      0.66        46\n",
      "        5.0       0.63      0.53      0.57        51\n",
      "        6.0       0.60      0.49      0.54        51\n",
      "        7.0       0.56      0.56      0.56        36\n",
      "        8.0       0.70      0.66      0.68        47\n",
      "        9.0       0.49      0.81      0.61        32\n",
      "\n",
      "avg / total       0.61      0.60      0.60       450\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "report_teste(pred_test,'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustando o classificador com Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "cv_kfold = KFold(10, shuffle = False)\n",
    "\n",
    "param_grid = {\"n_neighbors\": range(1, 50),\n",
    "                 \"weights\": [\"uniform\", \"distance\"],\n",
    "                 \"metric\": [\"euclidean\", \"manhattan\"]} #\"chebyshev\", \"minkowski\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_knn = GridSearchCV(fit_knn,\n",
    "                     cv = cv_kfold,\n",
    "                     param_grid = param_grid, \n",
    "                     n_jobs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=3,\n",
       "       param_grid={'n_neighbors': range(1, 50), 'weights': ['uniform', 'distance'], 'metric': ['euclidean', 'manhattan']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_knn.fit(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metric': 'manhattan', 'n_neighbors': 7, 'weights': 'distance'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_knn.set_params(n_neighbors = 7,\n",
    "                      metric = 'manhattan',\n",
    "                      weights = 'distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=7, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_knn.fit(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados Conjunto Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador KNN com Grid Search:\n",
      "Acurácia para o treino é  1.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      1.00      1.00       103\n",
      "        1.0       1.00      1.00      1.00        98\n",
      "        2.0       1.00      1.00      1.00       111\n",
      "        3.0       1.00      1.00      1.00       105\n",
      "        4.0       1.00      1.00      1.00       104\n",
      "        5.0       1.00      1.00      1.00        97\n",
      "        6.0       1.00      1.00      1.00        98\n",
      "        7.0       1.00      1.00      1.00       111\n",
      "        8.0       1.00      1.00      1.00       106\n",
      "        9.0       1.00      1.00      1.00       117\n",
      "\n",
      "avg / total       1.00      1.00      1.00      1050\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "pred_train2 = fit_knn.predict(X_treino)\n",
    "report_treino(pred_train2, 'KNN com Grid Search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resultados conjunto teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador KNN com Grid Search:\n",
      "Acurácia para o treino é  0.6533333333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.64      0.71        47\n",
      "        1.0       0.62      0.63      0.62        51\n",
      "        2.0       0.52      0.52      0.52        42\n",
      "        3.0       0.79      0.66      0.72        47\n",
      "        4.0       0.68      0.61      0.64        46\n",
      "        5.0       0.70      0.61      0.65        51\n",
      "        6.0       0.71      0.59      0.65        51\n",
      "        7.0       0.60      0.75      0.67        36\n",
      "        8.0       0.67      0.79      0.73        47\n",
      "        9.0       0.50      0.81      0.62        32\n",
      "\n",
      "avg / total       0.67      0.65      0.65       450\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "predictions_fit_knn = fit_knn.predict(X_teste)\n",
    "report_teste(predictions_fit_knn, 'KNN com Grid Search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_knn = fit_knn.predict(X_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0  2  0  0  4  1  3  5  2]\n",
      " [ 0 32  4  0  7  0  1  0  0  7]\n",
      " [ 1  1 22  2  0  4  4  5  0  3]\n",
      " [ 0  7  3 31  3  0  0  1  0  2]\n",
      " [ 0  5  1  2 28  0  0  1  0  9]\n",
      " [ 3  6  3  1  0 31  1  2  2  2]\n",
      " [ 0  0  6  0  3  3 30  3  6  0]\n",
      " [ 2  0  0  0  0  1  1 27  5  0]\n",
      " [ 1  0  1  1  0  0  4  2 37  1]\n",
      " [ 1  1  0  2  0  1  0  1  0 26]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_teste, predictions_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aqui está a nossa precisão média no conjunto de testes: 0.653\n"
     ]
    }
   ],
   "source": [
    "accuracy_knn = fit_knn.score(X_teste, y_teste)\n",
    "\n",
    "print(\"Aqui está a nossa precisão média no conjunto de testes: {0:.3f}\".format(accuracy_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A taxa de erro de teste para o nosso modelo é:  0.347\n"
     ]
    }
   ],
   "source": [
    "test_error_rate_knn = 1 - accuracy_knn\n",
    "print(\"A taxa de erro de teste para o nosso modelo é: {0: .3f}\" .format(test_error_rate_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curva ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_prob = fit_knn.predict_proba(X_teste)[:, 1]\n",
    "\n",
    "fpr2, tpr2, _ = roc_curve(y_teste,\n",
    "                          predictions_prob,\n",
    "                          pos_label = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_knn = auc(fpr2, tpr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr2, tpr2, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % auc_knn)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falso Positivo')\n",
    "plt.ylabel('Taxa de Verdadeiro Positivo')\n",
    "plt.title('Receiver operating characteristic ')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador KNN:\n",
      "Acurácia para o treino é  0.6533333333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.79      0.64      0.71        47\n",
      "        1.0       0.62      0.63      0.62        51\n",
      "        2.0       0.52      0.52      0.52        42\n",
      "        3.0       0.79      0.66      0.72        47\n",
      "        4.0       0.68      0.61      0.64        46\n",
      "        5.0       0.70      0.61      0.65        51\n",
      "        6.0       0.71      0.59      0.65        51\n",
      "        7.0       0.60      0.75      0.67        36\n",
      "        8.0       0.67      0.79      0.73        47\n",
      "        9.0       0.50      0.81      0.62        32\n",
      "\n",
      "avg / total       0.67      0.65      0.65       450\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "    report_teste(predictions_knn, 'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validação Cruzada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 0:100].values\n",
    "y = dataset['target'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6113333333333333, 0.03026549190084312)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=42)\n",
    "model = KNeighborsClassifier()\n",
    "scoring = 'accuracy'\n",
    "results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "results.mean(), results.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6113333333333333"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KNeighborsClassifier()\n",
    "accuracies = cross_val_score(model, X=X, y=y, cv=LeaveOneOut())\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6102222222222221"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "cv_repeat = RepeatedKFold(n_splits=6, n_repeats=3, random_state=42)\n",
    "model = KNeighborsClassifier()\n",
    "accuracies = cross_val_score(model, X=X, y=y, cv=cv_repeat)\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando as k primeiras observações para treino e o restante para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treino = dataset.iloc[0:499, 0:99].values\n",
    "y_treino = dataset.iloc[0:499, 100].values\n",
    "\n",
    "\n",
    "X_teste = dataset.iloc[500:1500, 0:99].values\n",
    "y_teste = dataset.iloc[500:1500, 100].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=5,\n",
    "                                  weights='uniform',\n",
    "                                  algorithm='auto',\n",
    "                                  leaf_size=30,\n",
    "                                  p=2,\n",
    "                                  metric='minkowski',\n",
    "                                  metric_params=None,\n",
    "                                  n_jobs=1)\n",
    "clf.fit(X_treino, y_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precisão do classificador no Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_teste = clf.predict(X_teste)\n",
    "pred_treino = clf.predict(X_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador KNN:\n",
      "Acurácia para o treino é  0.751503006012024\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.80      0.77        55\n",
      "        1.0       0.62      0.85      0.72        48\n",
      "        2.0       0.61      0.56      0.58        45\n",
      "        3.0       0.87      0.78      0.82        58\n",
      "        4.0       0.81      0.71      0.76        48\n",
      "        5.0       0.88      0.63      0.73        46\n",
      "        6.0       0.67      0.64      0.65        47\n",
      "        7.0       0.81      0.69      0.74        51\n",
      "        8.0       0.86      0.88      0.87        41\n",
      "        9.0       0.75      0.93      0.83        60\n",
      "\n",
      "avg / total       0.76      0.75      0.75       499\n",
      " None\n"
     ]
    }
   ],
   "source": [
    " report_treino(pred_treino, 'KNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para o classificador KNN:\n",
      "Acurácia para o treino é  0.562\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.77      0.60        95\n",
      "        1.0       0.46      0.58      0.52       101\n",
      "        2.0       0.51      0.49      0.50       108\n",
      "        3.0       0.72      0.45      0.55        94\n",
      "        4.0       0.63      0.52      0.57       102\n",
      "        5.0       0.68      0.37      0.48       102\n",
      "        6.0       0.49      0.46      0.48       102\n",
      "        7.0       0.60      0.53      0.56        96\n",
      "        8.0       0.76      0.60      0.67       111\n",
      "        9.0       0.52      0.89      0.66        89\n",
      "\n",
      "avg / total       0.59      0.56      0.56      1000\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "report_teste(pred_teste, 'KNN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
